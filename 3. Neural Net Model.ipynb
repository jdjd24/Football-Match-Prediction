{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import Tensor\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('x_train_full.csv')\n",
    "x_val = pd.read_csv('x_val_full.csv')\n",
    "x_test = pd.read_csv('x_test_full.csv')\n",
    "y_train = pd.read_csv('./Data with correct player_atts/y_train.csv')\n",
    "y_val = pd.read_csv('./Data with correct player_atts/y_val.csv')\n",
    "y_test = pd.read_csv('./Data with correct player_atts/y_test.csv')\n",
    "\n",
    "x_full = pd.concat([x_train,x_val,x_test], axis = 0)\n",
    "y_full = pd.concat([y_train,y_val,y_test], axis = 0)\n",
    "\n",
    "match = pd.read_csv('./Data/Match.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full = x_full.merge(match[['match_api_id','league_id']], how = 'left', on ='match_api_id')\n",
    "y_full = y_full.merge(match[['match_api_id','league_id']], how = 'left', on = 'match_api_id')\n",
    "\n",
    "assert (x_full.match_api_id == y_full.match_api_id).sum() == len(x_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {1:0, 0:1, -1:2 }\n",
    "y_full.loc[:,'target'] = y_full['result'].map(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9614, 35)"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pl = x_full[x_full['league_id'].isin([1729,21518,7809,10257,4769])].drop(columns ='league_id')\n",
    "y_pl = y_full[y_full['league_id'].isin([1729,21518,7809,10257,4769])].drop(columns ='league_id')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_pl,y_pl, test_size = 0.1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train,y_train, test_size = 0.12)\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating different training set for ensemble learning\n",
    "\n",
    "x1, y1 = resample(x_train, y_train, random_state=0)\n",
    "x2, y2 = resample(x_train, y_train, random_state=1)\n",
    "x3, y3 = resample(x_train, y_train, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler = preprocessing.StandardScaler().fit(x_train.iloc[:,1:])\n",
    "\n",
    "# x_train = pd.DataFrame(StandardScaler.transform(x_train.iloc[:,1:]), index = x_train['match_api_id'], columns = x_train.iloc[:,1:].columns)\n",
    "# x_val = pd.DataFrame(StandardScaler.transform(x_val.iloc[:,1:]), index = x_val['match_api_id'], columns = x_val.iloc[:,1:].columns)\n",
    "# x_test = pd.DataFrame(min_max_scaler.transform(x_test.iloc[:,1:]), index = x_test['match_api_id'], columns = x_test.iloc[:,1:].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    0.458394\n",
       "-1    0.283025\n",
       " 0    0.258581\n",
       "Name: result, dtype: float64"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train['result'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Network\n",
    "input_size = len(x_train.drop(columns ='match_api_id').columns)\n",
    "hidden_sizes = [128,512,512,128,32,16]\n",
    "output_size = 3\n",
    "prob = 0.7\n",
    "# Build a feed-forward network\n",
    "LinearNN1 = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                          nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[0]),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                          nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[1]),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                        nn.ReLU(),\n",
    "                          nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[2]),\n",
    "                        nn.Linear(hidden_sizes[2], hidden_sizes[3]),\n",
    "                        nn.ReLU(),\n",
    "                          nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[3]),\n",
    "                        nn.Linear(hidden_sizes[3], hidden_sizes[4]),\n",
    "                        nn.ReLU(),\n",
    "                         nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[4]),\n",
    "                        nn.Linear(hidden_sizes[4], hidden_sizes[5]),\n",
    "                        nn.ReLU(),\n",
    "                         nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[5]),\n",
    "                         nn.Linear(hidden_sizes[5], output_size),\n",
    "                      nn.Softmax(dim = 1)\n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Network\n",
    "input_size = len(x_train.drop(columns ='match_api_id').columns)\n",
    "hidden_sizes = [64,128,256,128,32,16]\n",
    "output_size = 3\n",
    "prob = 0.7\n",
    "# Build a feed-forward network\n",
    "LinearNN2 = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                          nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[0]),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                          nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[1]),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                        nn.ReLU(),\n",
    "                          nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[2]),\n",
    "                        nn.Linear(hidden_sizes[2], hidden_sizes[3]),\n",
    "                        nn.ReLU(),\n",
    "                          nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[3]),\n",
    "                        nn.Linear(hidden_sizes[3], hidden_sizes[4]),\n",
    "                        nn.ReLU(),\n",
    "                         nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[4]),\n",
    "                        nn.Linear(hidden_sizes[4], hidden_sizes[5]),\n",
    "                        nn.ReLU(),\n",
    "                         nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[5]),\n",
    "                         nn.Linear(hidden_sizes[5], output_size),\n",
    "                      nn.Softmax(dim = 1)\n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Network\n",
    "input_size = len(x_train.drop(columns ='match_api_id').columns)\n",
    "hidden_sizes = [64,128,256,128,32,16,8]\n",
    "output_size = 3\n",
    "prob = 0.7\n",
    "# Build a feed-forward network\n",
    "LinearNN3 = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.Tanh(),\n",
    "                          nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[0]),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.Tanh(),\n",
    "                          nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[1]),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                        nn.Tanh(),\n",
    "                          nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[2]),\n",
    "                        nn.Linear(hidden_sizes[2], hidden_sizes[3]),\n",
    "                        nn.Tanh(),\n",
    "                          nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[3]),\n",
    "                        nn.Linear(hidden_sizes[3], hidden_sizes[4]),\n",
    "                        nn.ReLU(),\n",
    "                         nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[4]),\n",
    "                        nn.Linear(hidden_sizes[4], hidden_sizes[5]),\n",
    "                        nn.ReLU(),\n",
    "                         nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[5]),\n",
    "                            nn.Linear(hidden_sizes[5], hidden_sizes[6]),\n",
    "                        nn.ReLU(),\n",
    "                         nn.Dropout(p=prob),\n",
    "                         nn.BatchNorm1d(hidden_sizes[6]),\n",
    "                         nn.Linear(hidden_sizes[6], output_size),\n",
    "                      nn.Softmax(dim = 1)\n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 512 #len(x_train)\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "loss = 0\n",
    "losses = []\n",
    "losses_val = []\n",
    "counter = 0\n",
    "\n",
    "\n",
    "dataset = TensorDataset( Tensor(x_train.values), torch.Tensor(y_train['target'].values) )\n",
    "train_loader = DataLoader(dataset, batch_size = batch_size, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(LinearNN.parameters(), lr=learning_rate, betas=(0.9,0.999))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 60 , gamma = 0.1)\n",
    "mse = torch.nn.MSELoss()\n",
    "cross = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model\n",
    "\n",
    "t = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for x, y in iter(train_loader):\n",
    "        LinearNN.train()\n",
    "        LinearNN.zero_grad()\n",
    "        \n",
    "        y_pred = LinearNN(x)\n",
    "        y_val_pred = LinearNN(Tensor(x_val.values))\n",
    "\n",
    "        loss = cross(y_pred, y.long())\n",
    "        loss_val = cross(y_val_pred, Tensor(y_val['target'].values).long())\n",
    "\n",
    "        if counter % 500 ==0:\n",
    "            print('Loss after iteration {}: {}'.format(counter, loss.item()))\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        losses_val.append(loss_val.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        counter+=1 \n",
    "    counter+=1\n",
    "    scheduler.step()\n",
    "time.time()-t        \n",
    "        \n",
    "        \n",
    "print('Elapsed time: {} s'.format(time.time()-t))    \n",
    "print(loss.item())  \n",
    "plt.plot(range(len(losses)), losses, label = 'train')\n",
    "plt.plot(range(len(losses)), losses_val, label = 'val')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Eval \n",
    "\n",
    "LinearNN.eval()\n",
    "with torch.no_grad():\n",
    "    val_pred = LinearNN(Tensor(x_val.values))\n",
    "    train_pred = LinearNN(Tensor(x_train.values))\n",
    "y_val_pred = pd.Series(val_pred.max(1).indices).map({0:1,1:0,2:-1})\n",
    "y_train_pred = pd.Series(train_pred.max(1).indices).map({0:1,1:0,2:-1})\n",
    "\n",
    "print(sklearn.metrics.accuracy_score(y_train_pred, y_train['result']))\n",
    "print(sklearn.metrics.accuracy_score(y_val_pred, y_val['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    1008\n",
       "-1     457\n",
       "dtype: int64"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test \n",
    "\n",
    "LinearNN.eval()\n",
    "with torch.no_grad():\n",
    "    val_pred = LinearNN(Tensor(x_test.values))\n",
    "    \n",
    "y_val_pred = pd.Series(val_pred.max(1).indices).map({0:1,1:0,2:-1})\n",
    "sklearn.metrics.accuracy_score(y_val_pred, y_test['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    797\n",
       "-1    296\n",
       "dtype: int64"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_pred.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train,Y_train, X_val, Y_val,  epochs, batch_size, learning_rate):\n",
    "    StandardScaler = preprocessing.StandardScaler().fit(X_train.iloc[:,1:])\n",
    "    \n",
    "    X_train = pd.DataFrame(StandardScaler.transform(X_train.iloc[:,1:]), index = X_train['match_api_id'], columns = X_train.iloc[:,1:].columns)\n",
    "    X_val = pd.DataFrame(StandardScaler.transform(X_val.iloc[:,1:]), index = X_val['match_api_id'], columns = X_val.iloc[:,1:].columns)\n",
    "    \n",
    "    # Hyperparameters\n",
    "\n",
    "    batch_size = batch_size #len(x_train)\n",
    "    epochs = epochs\n",
    "    learning_rate = learning_rate\n",
    "    loss = 0\n",
    "    losses = []\n",
    "    losses_val = []\n",
    "    counter = 0\n",
    "    \n",
    "\n",
    "    dataset = TensorDataset( Tensor(X_train.values), torch.Tensor(Y_train['target'].values) )\n",
    "    train_loader = DataLoader(dataset, batch_size = batch_size, shuffle=False)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9,0.999))\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 25 , gamma = 0.1)\n",
    "    cross = nn.CrossEntropyLoss()\n",
    "    \n",
    "    #Train Model\n",
    "\n",
    "    t = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in iter(train_loader):\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "\n",
    "            y_pred = model(x)\n",
    "            y_val_pred = model(Tensor(X_val.values))\n",
    "\n",
    "            loss = cross(y_pred, y.long())\n",
    "            loss_val = cross(y_val_pred, Tensor(y_val['target'].values).long())\n",
    "\n",
    "            if counter % 500 ==0:\n",
    "                print('Loss after iteration {}: {}'.format(counter, loss.item()))\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            losses_val.append(loss_val.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            counter+=1 \n",
    "        counter+=1\n",
    "        scheduler.step()\n",
    "    time.time()-t        \n",
    "\n",
    "\n",
    "    print('Elapsed time: {} s'.format(time.time()-t))    \n",
    "    print(loss.item())  \n",
    "    plt.plot(range(len(losses)), losses, label = 'train')\n",
    "    plt.plot(range(len(losses)), losses_val, label = 'val')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.show()\n",
    "    \n",
    "    ##Eval \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(Tensor(X_val.values))\n",
    "        train_pred = model(Tensor(X_train.values))\n",
    "    y_val_pred = pd.Series(val_pred.max(1).indices).map({0:1,1:0,2:-1})\n",
    "    y_train_pred = pd.Series(train_pred.max(1).indices).map({0:1,1:0,2:-1})\n",
    "\n",
    "    print('Train accuracy: {}'.format(sklearn.metrics.accuracy_score(y_train_pred, Y_train['result'])))\n",
    "    print('Val accuracy: {}'.format(sklearn.metrics.accuracy_score(y_val_pred, Y_val['result'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.9999188184738159\n"
     ]
    }
   ],
   "source": [
    "train(LinearNN1, x1, y1, x_val, y_val,  5, 1024, 0.0001)\n",
    "train(LinearNN2, x2, y2, x_val, y_val,  5, 1024, 0.0001)\n",
    "train(LinearNN3, x3, y3, x_val, y_val,  5, 1024, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(LinearNN, './nn0.pt')\n",
    "# torch.save(LinearNN1, './nn1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, training_data, training_labels, x_val, y_val):\n",
    "    ensemble = []\n",
    "    val_pred_ensemble = 0\n",
    "    for i,model in enumerate(models):\n",
    "        StandardScaler = preprocessing.StandardScaler().fit(training_data[i].iloc[:,1:])\n",
    "        X_train = pd.DataFrame(StandardScaler.transform(training_data[i].iloc[:,1:]), index = training_data[i]['match_api_id'], columns = training_data[i].iloc[:,1:].columns)\n",
    "        X_val = pd.DataFrame(StandardScaler.transform(x_val.iloc[:,1:]), index = x_val['match_api_id'], columns = x_val.iloc[:,1:].columns)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(Tensor(X_val.values))\n",
    "            train_pred = model(Tensor(X_train.values))\n",
    "        y_val_pred = pd.Series(val_pred.max(1).indices).map({0:1,1:0,2:-1})\n",
    "        y_train_pred = pd.Series(train_pred.max(1).indices).map({0:1,1:0,2:-1})\n",
    "        \n",
    "        val_pred_ensemble += val_pred\n",
    "        print('model: {}'.format(i))\n",
    "        print('Train accuracy: {}'.format(sklearn.metrics.accuracy_score(y_train_pred, training_labels[i]['result'])))\n",
    "        print('Val accuracy: {}'.format(sklearn.metrics.accuracy_score(y_val_pred, y_val['result'])))\n",
    "        \n",
    "    global y_ens\n",
    "    y_ens = pd.Series(val_pred_ensemble.max(1).indices).map({0:1,1:0,2:-1})\n",
    "    print('Ensemble Val accuracy: {}'.format(sklearn.metrics.accuracy_score(y_ens, y_val['result'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 0\n",
      "Train accuracy: 0.5358851674641149\n",
      "Val accuracy: 0.524390243902439\n",
      "model: 1\n",
      "Train accuracy: 0.4491366756812981\n",
      "Val accuracy: 0.4413109756097561\n",
      "model: 2\n",
      "Train accuracy: 0.53505304763886\n",
      "Val accuracy: 0.5144817073170732\n",
      "Ensemble Val accuracy: 0.5274390243902439\n"
     ]
    }
   ],
   "source": [
    "ensemble_predict([LinearNN1,LinearNN2,LinearNN3], [x1,x2,x3], [y1,y2,y3], x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 0\n",
      "Train accuracy: 0.5358851674641149\n",
      "Val accuracy: 0.524390243902439\n",
      "model: 1\n",
      "Train accuracy: 0.53505304763886\n",
      "Val accuracy: 0.5144817073170732\n",
      "Ensemble Val accuracy: 0.5282012195121951\n"
     ]
    }
   ],
   "source": [
    "ensemble_predict([LinearNN1,LinearNN3], [x1,x3], [y1,y3], x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    904\n",
       "-1    408\n",
       "dtype: int64"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ens.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
